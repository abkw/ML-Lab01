#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(x=MSE,y=as.character(Features))
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
#linear regression
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X = cbind(1,X)
#MISSING: check formulas for linear regression and compute beta
print(Xpred1)
beta <- (solve(t(X)%*%X))%*%t(X)%*%Y
print(beta)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
print(X)
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
#Creating five folds for the parameters
#Looping through all folds
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
n=dim(X)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.2),replace = FALSE)
newX <- as.matrix(X[,which(model==1)])
# flds <- createFolds(newX, k = 5, list = TRUE, returnTrain = FALSE)
# id <- as.integer(unlist(flds[k]))
train <- newX[-id,]
test <- newX[id,]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
Ypred <- mylin(train, Y[-id] ,test)
Yp <- Y[id]
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(x=MSE,y=as.vector(Features))
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
#linear regression
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X = cbind(1,X)
#MISSING: check formulas for linear regression and compute beta
print(Xpred1)
beta <- (solve(t(X)%*%X))%*%t(X)%*%Y
print(beta)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
print(X)
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
#Creating five folds for the parameters
#Looping through all folds
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
n=dim(X)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.2),replace = FALSE)
newX <- as.matrix(X[,which(model==1)])
# flds <- createFolds(newX, k = 5, list = TRUE, returnTrain = FALSE)
# id <- as.integer(unlist(flds[k]))
train <- newX[-id,]
test <- newX[id,]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
Ypred <- mylin(train, Y[-id] ,test)
Yp <- Y[id]
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(x=MSE,y=as.vector(unlist(Features)))
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
#linear regression
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X = cbind(1,X)
#MISSING: check formulas for linear regression and compute beta
print(Xpred1)
beta <- (solve(t(X)%*%X))%*%t(X)%*%Y
print(beta)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
print(X)
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
#Creating five folds for the parameters
#Looping through all folds
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
n=dim(X)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.2),replace = FALSE)
newX <- as.matrix(X[,which(model==1)])
# flds <- createFolds(newX, k = 5, list = TRUE, returnTrain = FALSE)
# id <- as.integer(unlist(flds[k]))
train <- newX[-id,]
test <- newX[id,]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
Ypred <- mylin(train, Y[-id] ,test)
Yp <- Y[id]
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(x=MSE,y=as.character(Features))
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
#linear regression
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X = cbind(1,X)
#MISSING: check formulas for linear regression and compute beta
print(Xpred1)
beta <- (solve(t(X)%*%X))%*%t(X)%*%Y
print(beta)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
print(X)
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
#Creating five folds for the parameters
#Looping through all folds
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
n=dim(X)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.2),replace = FALSE)
newX <- as.matrix(X[,which(model==1)])
# flds <- createFolds(newX, k = 5, list = TRUE, returnTrain = FALSE)
# id <- as.integer(unlist(flds[k]))
train <- newX[-id,]
test <- newX[id,]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
Ypred <- mylin(train, Y[-id] ,test)
Yp <- Y[id]
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(x=MSE,y=as.character.factor(Features))
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
#linear regression
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X = cbind(1,X)
#MISSING: check formulas for linear regression and compute beta
print(Xpred1)
beta <- (solve(t(X)%*%X))%*%t(X)%*%Y
print(beta)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
print(X)
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
#Creating five folds for the parameters
#Looping through all folds
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
n=dim(X)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.2),replace = FALSE)
newX <- as.matrix(X[,which(model==1)])
# flds <- createFolds(newX, k = 5, list = TRUE, returnTrain = FALSE)
# id <- as.integer(unlist(flds[k]))
train <- newX[-id,]
test <- newX[id,]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
Ypred <- mylin(train, Y[-id] ,test)
Yp <- Y[id]
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(factor(Features),MSE)
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
#linear regression
mylin=function(X,Y, Xpred){
Xpred1=cbind(1,Xpred)
X = cbind(1,X)
#MISSING: check formulas for linear regression and compute beta
print(Xpred1)
beta <- (solve(t(X)%*%X))%*%t(X)%*%Y
print(beta)
Res=Xpred1%*%beta
return(Res)
}
myCV=function(X,Y,Nfolds){
print(X)
n=length(Y)
p=ncol(X)
set.seed(12345)
ind=sample(n,n)
X1=X[ind,]
Y1=Y[ind]
sF=floor(n/Nfolds)
MSE=numeric(2^p-1)
Nfeat=numeric(2^p-1)
Features=list()
curr=0
#we assume 5 features.
for (f1 in 0:1)
for (f2 in 0:1)
for(f3 in 0:1)
for(f4 in 0:1)
for(f5 in 0:1){
model= c(f1,f2,f3,f4,f5)
if (sum(model)==0) next()
SSE=0
#Creating five folds for the parameters
#Looping through all folds
for (k in 1:Nfolds){
#MISSING: compute which indices should belong to current fold
n=dim(X)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.2),replace = FALSE)
newX <- as.matrix(X[,which(model==1)])
# flds <- createFolds(newX, k = 5, list = TRUE, returnTrain = FALSE)
# id <- as.integer(unlist(flds[k]))
train <- newX[-id,]
test <- newX[id,]
#MISSING: implement cross-validation for model with features in "model" and iteration i.
Ypred <- mylin(train, Y[-id] ,test)
Yp <- Y[id]
#MISSING: Get the predicted values for fold 'k', Ypred, and the original values for folf 'k', Yp.
SSE=SSE+sum((Ypred-Yp)^2)
}
curr=curr+1
MSE[curr]=SSE/n
Nfeat[curr]=sum(model)
Features[[curr]]=model
}
#MISSING: plot MSE against number of features
plot(factor(as.character(Features)),MSE)
i=which.min(MSE)
return(list(CV=MSE[i], Features=Features[[i]]))
}
myCV(as.matrix(swiss[,2:6]), swiss[[1]], 5)
# importing xlsx files
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
names(data)
# importing xlsx files
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
names(data)
print(data)
# importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#1) importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#From the plot, the data could be described very well in a linear model, and it appears
#that moisture and protein have a positive relation
#2) ð‘€i is our model ~ N(M,Sigma^2)
#1) importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#From the plot, the data could be described very well in a linear model, and it appears
#that moisture and protein have a positive relation
#2) ð‘€i is our model ~ N(M,Sigma^2)
#Mi = B0 + B1X + B2X^2 + .... + BiX^i
#it is appropriate to use MSE criterion when fitting this model because the moisture is
#normally distributed and and the expected Moisture is a polynomial function of Protein.
#3)Dividing the data into training and testing, and fitting the models
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
logiModel <- glm(Fat ~ . , family = binomial, data = train)
summary(logiModel)
# Prediction for testing data
glm.probs <- predict(logiModel, newData = test, type = 'response')
?glm
#1) importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#From the plot, the data could be described very well in a linear model, and it appears
#that moisture and protein have a positive relation
#2) ð‘€i is our model ~ N(M,Sigma^2)
#Mi = B0 + B1X + B2X^2 + .... + BiX^i
#it is appropriate to use MSE criterion when fitting this model because the moisture is
#normally distributed and and the expected Moisture is a polynomial function of Protein.
#3)Dividing the data into training and testing, and fitting the models
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
logiModel <- glm(Fat ~ . , family = data , data = train)
summary(logiModel)
# Prediction for testing data
glm.probs <- predict(logiModel, newData = test, type = 'response')
#1) importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#From the plot, the data could be described very well in a linear model, and it appears
#that moisture and protein have a positive relation
#2) ð‘€i is our model ~ N(M,Sigma^2)
#Mi = B0 + B1X + B2X^2 + .... + BiX^i
#it is appropriate to use MSE criterion when fitting this model because the moisture is
#normally distributed and and the expected Moisture is a polynomial function of Protein.
#3)Dividing the data into training and testing, and fitting the models
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
logiModel <- glm(Fat ~ . , data = train)
summary(logiModel)
# Prediction for testing data
glm.probs <- predict(logiModel, newData = test, type = 'response')
glm.probs
#1) importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#From the plot, the data could be described very well in a linear model, and it appears
#that moisture and protein have a positive relation
#2) ð‘€i is our model ~ N(M,Sigma^2)
#Mi = B0 + B1X + B2X^2 + .... + BiX^i
#it is appropriate to use MSE criterion when fitting this model because the moisture is
#normally distributed and and the expected Moisture is a polynomial function of Protein.
#3)Dividing the data into training and testing, and fitting the models
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
logiModel <- glm(Fat ~ . , data = train)
summary(logiModel)
# Prediction for testing data
glm.probs <- predict(logiModel, newData = test, type = 'response')
erro <- logiModel$fitted.values - glm.probs
errorCondition()
error
erro
?predict
formals(predict)
example("predict")
logiModel$coefficients
logiModel$residuals
logiModel$effects
#1) importing xlsx files for the data and plotting the moisture versus protein
data <- read_excel("D:/Desktop/Machine Learning/Machine Learning/lab01/tecator.xlsx")
plot(data$Moisture,data$Protein)
#From the plot, the data could be described very well in a linear model, and it appears
#that moisture and protein have a positive relation
#2) ð‘€i is our model ~ N(M,Sigma^2)
#Mi = B0 + B1X + B2X^2 + .... + BiX^i
#it is appropriate to use MSE criterion when fitting this model because the moisture is
#normally distributed and and the expected Moisture is a polynomial function of Protein.
#3)Dividing the data into training and testing, and fitting the models
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
logiModel <- glm(Fat ~ . , data = train)
summary(logiModel)
# Prediction for testing data
glm.probs <- predict(logiModel, newData = test, type = 'response')
erro <- logiModel$fitted.values - glm.probs
table(glm.probs,train$Fat)
table(glm.probs,test$Fat)
